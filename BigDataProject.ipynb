{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2NXKEQ_55Xm",
        "outputId": "f97823ed-0eb8-41c2-a6b3-f6f860470d5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.11)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.2.0)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.8)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.4)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.28.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.2.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.13.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.4.26)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Requirement already satisfied: ndjson in /usr/local/lib/python3.11/dist-packages (0.3.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb\n",
        "import json\n",
        "!pip install ndjson\n",
        "!mkdir -p quickdraw_data\n",
        "import os\n",
        "import wandb\n",
        "import ndjson\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image, ImageDraw\n",
        "\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image, ImageDraw\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Les 10 premières catégories disponibles dans le dataset\n",
        "categories = [\n",
        "    \"donut\", \"airplane\", \"angel\", \"axe\", \"banana\",\n",
        "    \"bridge\", \"cup\", \"apple\", \"door\", \"mountain\"\n",
        "]\n",
        "\n",
        "# Création du dossier de données\n",
        "os.makedirs(\"quickdraw_data\", exist_ok=True)\n",
        "\n",
        "# Téléchargement automatique via wget\n",
        "for category in categories:\n",
        "    url = f\"https://storage.googleapis.com/quickdraw_dataset/full/simplified/{category}.ndjson\"\n",
        "    filename = f\"quickdraw_data/{category}.ndjson\"\n",
        "    if not os.path.exists(filename):\n",
        "        print(f\"Téléchargement : {category}\")\n",
        "        !wget -q -O {filename} {url}\n",
        "    else:\n",
        "        print(f\"Déjà téléchargé : {category}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80KtH_ay7P5p",
        "outputId": "b4c7a6f9-f8ce-4cbb-9b37-1c280b6f8d23"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Déjà téléchargé : donut\n",
            "Déjà téléchargé : airplane\n",
            "Déjà téléchargé : angel\n",
            "Déjà téléchargé : axe\n",
            "Déjà téléchargé : banana\n",
            "Déjà téléchargé : bridge\n",
            "Déjà téléchargé : cup\n",
            "Déjà téléchargé : apple\n",
            "Déjà téléchargé : door\n",
            "Déjà téléchargé : mountain\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Parser et convertir les 1000 premiers dessins reconnus des 10 catégories déjà téléchargées (en .ndjson)\n",
        "# en images 28x28, et les stocker dans deux arrays : images (numpy) et labels (str).\n",
        "\n",
        "# Fonction pour convertir un dessin (strokes) en image PIL 28x28\n",
        "def draw_strokes(drawing, size=28, lw=3):\n",
        "    img = Image.new(\"L\", (256, 256), color=0)  # 256x256 pour avoir de la marge\n",
        "    draw = ImageDraw.Draw(img)\n",
        "\n",
        "    for stroke in drawing:\n",
        "        points = list(zip(stroke[0], stroke[1]))\n",
        "        draw.line(points, fill=255, width=lw)\n",
        "\n",
        "    img = img.resize((size, size), Image.Resampling.LANCZOS)\n",
        "    return np.array(img)\n",
        "\n",
        "\n",
        "# Parser et convertir les 100 premiers dessins reconnus\n",
        "images = []\n",
        "labels = []\n",
        "\n",
        "for category in categories:\n",
        "    path = f\"quickdraw_data/{category}.ndjson\"\n",
        "    try:\n",
        "        with open(path) as f:\n",
        "            data = ndjson.load(f)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Fichier manquant pour {category}, ignoré.\")\n",
        "        continue\n",
        "\n",
        "    count = 0\n",
        "    for sample in data:\n",
        "        if sample[\"recognized\"]:\n",
        "            img = draw_strokes(sample[\"drawing\"])\n",
        "            images.append(img)\n",
        "            labels.append(category)\n",
        "            count += 1\n",
        "        if count >= 1000:  # Load 1000 samples per category\n",
        "            break\n",
        "\n",
        "images = np.array(images)\n",
        "labels = np.array(labels)\n",
        "\n",
        "print(\"Total images :\", images.shape)\n",
        "print(\"Total labels :\", labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVbfuvjK7a4g",
        "outputId": "7ed6ae32-3eac-4c4b-fb28-7335c0f18fc2"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total images : (10000, 28, 28)\n",
            "Total labels : (10000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualisez une image random\n",
        "import random\n",
        "\n",
        "idx = random.randint(0, len(images) - 1)\n",
        "plt.imshow(images[idx], cmap=\"gray\")\n",
        "plt.title(labels[idx])\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "t6I5xNdN7y_L",
        "outputId": "dffc61be-5250-4b4d-d97a-41934e7e7f1a"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalisation : on ramène les valeurs des pixels entre 0 et 1\n",
        "images = images.astype(\"float32\") / 255.0\n",
        "images = np.expand_dims(images, axis=1)  # PyTorch : (N, 1, 28, 28)\n",
        "\n",
        "# Encodage des labels (ex: 'cat' -> 3, 'dog' -> 5, etc.)\n",
        "label_encoder = LabelEncoder()\n",
        "labels_encoded = label_encoder.fit_transform(labels)\n",
        "\n",
        "print(\"Catégories encodées :\", list(label_encoder.classes_))\n",
        "print(\"Nombre de classes :\", len(label_encoder.classes_))\n",
        "\n",
        "\n",
        "# Split des données\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(images, labels_encoded, test_size=0.2, stratify=labels_encoded, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)\n",
        "\n",
        "# Conversion en tenseurs\n",
        "X_train = torch.tensor(X_train)\n",
        "X_val   = torch.tensor(X_val)\n",
        "X_test  = torch.tensor(X_test)\n",
        "\n",
        "y_train = torch.tensor(y_train)\n",
        "y_val   = torch.tensor(y_val)\n",
        "y_test  = torch.tensor(y_test)\n",
        "\n",
        "# Création des datasets\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "val_dataset   = TensorDataset(X_val, y_val)\n",
        "test_dataset  = TensorDataset(X_test, y_test)\n",
        "\n",
        "# Création des DataLoaders\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader   = DataLoader(val_dataset, batch_size=batch_size)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "print(\"Données prêtes pour l'entraînement.\")\n",
        "\n",
        "#On a 8 classes alors qu'on devrait en avoir 10 il faut donc prendre la variable \"classes\" dans le modèle CNN pour être sûr de ne pas avoir des classes des fichiers .ndjson sont en erreur"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yxLJTUID9yFp",
        "outputId": "d3a6e047-c19b-4538-fb56-6d42499e1bfb"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Catégories encodées : [np.str_('airplane'), np.str_('angel'), np.str_('apple'), np.str_('axe'), np.str_('banana'), np.str_('bridge'), np.str_('cup'), np.str_('donut'), np.str_('door'), np.str_('mountain')]\n",
            "Nombre de classes : 10\n",
            "Données prêtes pour l'entraînement.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CNN simple avec PyTorch, inspiré de LeNet-5, adapté aux images 28×28 en niveaux de gris\n",
        "class BetterCNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(BetterCNN, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=3, padding=1),  # (1, 28, 28) -> (32, 28, 28)\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 32, kernel_size=3, padding=1),  # (32, 28, 28)\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),  # (32, 14, 14)\n",
        "            nn.Dropout(0.3),\n",
        "\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),  # (64, 14, 14)\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),  # (64, 14, 14)\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),  # (64, 7, 7)\n",
        "            nn.Dropout(0.5),\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(64 * 7 * 7, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "model = BetterCNN(num_classes=len(label_encoder.classes_))"
      ],
      "metadata": {
        "id": "ywxs8oAM_mkt"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transforme les tableaux images et labels en un Dataset PyTorch, puis les séparer en train, val et test\n",
        "\n",
        "class QuickDrawDataset(Dataset):\n",
        "    def __init__(self, images, labels, transform=None):\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "        self.transform = transforms.Compose([\n",
        "               transforms.ToPILImage(),\n",
        "               transforms.RandomRotation(10),  # Rotation aléatoire de 10 degrés\n",
        "               transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),  # Translation aléatoire\n",
        "               transforms.ToTensor()\n",
        "           ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # The line below was causing the problem\n",
        "        # image = torch.tensor(image).unsqueeze(0)  # Shape: (1, 28, 28)\n",
        "        # We already have (1, 28, 28) from previous operations\n",
        "        image = torch.tensor(image)  # Keep the original image shape (1, 28, 28)\n",
        "        label = torch.tensor(label).long()\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n"
      ],
      "metadata": {
        "id": "vFWXovPzAKQQ"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Séparation train/val/test\n",
        "\n",
        "# Encodage des labels texte en entiers\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(labels)\n",
        "\n",
        "# Split 80% train, 10% val, 10% test\n",
        "# Use labels_np instead of labels_encoded for stratify\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(images, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
        "\n",
        "print(f\"Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UeF-yfilBEaQ",
        "outputId": "edbd1662-7467-434b-c239-c192c0818d0e"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 8000, Val: 1000, Test: 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Création des DataLoader\n",
        "batch_size = 64\n",
        "\n",
        "train_dataset = QuickDrawDataset(X_train, y_train)\n",
        "val_dataset = QuickDrawDataset(X_val, y_val)\n",
        "test_dataset = QuickDrawDataset(X_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "X27i2FyDBT33"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialisation de l’entraînement - Définir modèle, loss et optimizer\n",
        "model = BetterCNN(num_classes=len(label_encoder.classes_)).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5) # L2 regularization\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=1)  # Reduce LR if val loss plateaus"
      ],
      "metadata": {
        "id": "MBNiH-E9Bd1K"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Intégration avec Weights & Biases - Initialisation wandb\n",
        "\n",
        "wandb.init(project=\"quickdraw-cnn\", config={\n",
        "    \"architecture\": \"BetterCNN\",\n",
        "    \"dataset\": \"QuickDraw (20 classes)\",\n",
        "    \"epochs\": 10,\n",
        "    \"batch_size\": batch_size,\n",
        "    \"learning_rate\": 1e-3\n",
        "})\n",
        "\n",
        "wandb.watch(model, log=\"all\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "5cjOXZxmBxMx",
        "outputId": "388f3bec-b252-4936-e3b5-21e0989bd920"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250520_001336-r5rco0re</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/georjonremy-t-l-com-saint-etienne/quickdraw-cnn/runs/r5rco0re' target=\"_blank\">bumbling-gorge-15</a></strong> to <a href='https://wandb.ai/georjonremy-t-l-com-saint-etienne/quickdraw-cnn' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/georjonremy-t-l-com-saint-etienne/quickdraw-cnn' target=\"_blank\">https://wandb.ai/georjonremy-t-l-com-saint-etienne/quickdraw-cnn</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/georjonremy-t-l-com-saint-etienne/quickdraw-cnn/runs/r5rco0re' target=\"_blank\">https://wandb.ai/georjonremy-t-l-com-saint-etienne/quickdraw-cnn/runs/r5rco0re</a>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# boucle entrainement\n",
        "\n",
        "best_val_acc = 0\n",
        "patience = 2\n",
        "wait = 0\n",
        "\n",
        "train_accs, val_accs = [], []\n",
        "\n",
        "for epoch in range(20):\n",
        "    model.train()\n",
        "    correct = total = 0\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "    train_acc = 100 * correct / total\n",
        "    train_accs.append(train_acc)\n",
        "\n",
        "    model.eval()\n",
        "    correct = total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "    val_acc = 100 * correct / total\n",
        "    val_accs.append(val_acc)\n",
        "\n",
        "    print(f\"[{epoch+1}/20] Train Acc: {train_acc:.2f}% | Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "    scheduler.step(val_acc)\n",
        "\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        wait = 0\n",
        "        torch.save(model.state_dict(), \"best_model.pth\")\n",
        "    else:\n",
        "        wait += 1\n",
        "        if wait >= patience:\n",
        "            print(\"Early stopping.\")\n",
        "            break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PcCmcsz9CrVB",
        "outputId": "0abb9efe-bde0-4b2f-cbad-3efa01468bd9"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1/20] Train Acc: 64.59% | Val Acc: 81.20%\n",
            "[2/20] Train Acc: 80.70% | Val Acc: 87.40%\n",
            "[3/20] Train Acc: 84.29% | Val Acc: 88.50%\n",
            "[4/20] Train Acc: 86.04% | Val Acc: 90.40%\n",
            "[5/20] Train Acc: 87.20% | Val Acc: 90.10%\n",
            "[6/20] Train Acc: 87.15% | Val Acc: 90.70%\n",
            "[7/20] Train Acc: 86.90% | Val Acc: 91.10%\n",
            "[8/20] Train Acc: 86.69% | Val Acc: 90.40%\n",
            "[9/20] Train Acc: 86.84% | Val Acc: 90.70%\n",
            "Early stopping.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Évaluation sur le test set\n",
        "\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "test_acc = 100 * correct / total\n",
        "print(f\"Test Accuracy: {test_acc:.2f}%\")\n",
        "wandb.log({\"test_acc\": test_acc})\n",
        "\n",
        "# fonction d’affichage de courbes\n",
        "\n",
        "def plot_training_history(history):\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history[\"accuracy\"], label=\"Train\")\n",
        "    plt.plot(history.history[\"val_accuracy\"], label=\"Val\")\n",
        "    plt.title(\"Accuracy\")\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history[\"loss\"], label=\"Train\")\n",
        "    plt.plot(history.history[\"val_loss\"], label=\"Val\")\n",
        "    plt.title(\"Loss\")\n",
        "    plt.legend()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ded_K-5eIYJt",
        "outputId": "8868deb9-594b-4f57-cf21-0c5334d91241"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 90.40%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Sauvegarde du modèle entraîné (.pth)\n",
        "\n",
        "model_path = \"quickdraw_cnn.pth\"\n",
        "torch.save(model.state_dict(), model_path)\n",
        "print(f\"Modèle sauvegardé dans {model_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Am8Z4daWIcfY",
        "outputId": "4aa66c97-786b-41a1-e258-8d2056972f5d"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modèle sauvegardé dans quickdraw_cnn.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Arrêt du tracking Weights & Biases\n",
        "\n",
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "J59EaLqzIkeW",
        "outputId": "e88fc3dc-3d6d-4990-fc71-f7f90c068dfe"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>test_acc</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test_acc</td><td>90.4</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">bumbling-gorge-15</strong> at: <a href='https://wandb.ai/georjonremy-t-l-com-saint-etienne/quickdraw-cnn/runs/r5rco0re' target=\"_blank\">https://wandb.ai/georjonremy-t-l-com-saint-etienne/quickdraw-cnn/runs/r5rco0re</a><br> View project at: <a href='https://wandb.ai/georjonremy-t-l-com-saint-etienne/quickdraw-cnn' target=\"_blank\">https://wandb.ai/georjonremy-t-l-com-saint-etienne/quickdraw-cnn</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250520_001336-r5rco0re/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jgZ6APxBIoMQ"
      },
      "execution_count": 147,
      "outputs": []
    }
  ]
}